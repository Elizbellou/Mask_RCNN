{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "custom.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elizbellou/Mask_RCNN/blob/main/custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5I0mhTR_UvG"
      },
      "source": [
        "\r\n"
      ],
      "id": "_5I0mhTR_UvG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N4gFiBUQQv0"
      },
      "source": [
        "\r\n"
      ],
      "id": "1N4gFiBUQQv0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ov0SyLlRSk3",
        "outputId": "cece970a-887d-4cc0-8d21-1453ab459505"
      },
      "source": [
        ""
      ],
      "id": "9ov0SyLlRSk3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Woo! You got the right kind of GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__5rAmlvj7d-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04eaead-1959-4b33-849f-d9242dd66476"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\r\n",
        "!pip install gputil\r\n",
        "!pip install psutil\r\n",
        "!pip install humanize\r\n",
        "import psutil\r\n",
        "import humanize\r\n",
        "import os\r\n",
        "import GPUtil as GPU\r\n",
        "GPUs = GPU.getGPUs()\r\n",
        "gpu = GPUs[0]\r\n"
      ],
      "id": "__5rAmlvj7d-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=59371be7c46ef3560b11bf3d5ca2db82c4f6910315dc75549c1e8d275bc86abf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "featured-martial"
      },
      "source": [
        ""
      ],
      "id": "featured-martial",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWnCAa1IuBG6"
      },
      "source": [
        ""
      ],
      "id": "WWnCAa1IuBG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3sNXeN_ZLr5",
        "outputId": "a46061c4-6839-4d6f-f95a-5cc230995faa"
      },
      "source": [
        "!pip install keras==2.1.6\r\n",
        "%tensorflow_version 2.x\r\n"
      ],
      "id": "c3sNXeN_ZLr5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\r\u001b[K     |█                               | 10kB 14.1MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 11.6MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.19.5)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AREf6u-7eWhW",
        "outputId": "c91a8b9d-b045-4e3a-c84a-bc7787c61d43"
      },
      "source": [
        "import tensorflow\r\n",
        "print(tensorflow.__version__)"
      ],
      "id": "AREf6u-7eWhW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ResD26d473",
        "outputId": "7633033a-d131-48e6-800b-295014a763ab"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version\r\n"
      ],
      "id": "L6ResD26d473",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeIm2ATUIUAO",
        "outputId": "6c1295fa-15b9-40c8-e4f5-5362e9f0b289"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "NeIm2ATUIUAO",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Mar 17 14:36:44 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D2hc2uLIe7W"
      },
      "source": [
        "import pynvml\r\n",
        "\r\n",
        "\r\n",
        "pynvml.nvmlInit()\r\n",
        "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\r\n",
        "device_name = pynvml.nvmlDeviceGetName(handle)\r\n",
        "\r\n",
        "if device_name != b'Tesla T4':\r\n",
        "  raise Exception(\"\"\"\r\n",
        "    Unfortunately this instance does not have a T4 GPU.\r\n",
        "    \r\n",
        "    Please make sure you've configured Colab to request a GPU instance type.\r\n",
        "    \r\n",
        "    Sometimes Colab allocates a Tesla K80 instead of a T4. Resetting the instance.\r\n",
        "\r\n",
        "    If you get a K80 GPU, try Runtime -> Reset all runtimes...\r\n",
        "  \"\"\")\r\n",
        "else:\r\n",
        "  print('Woo! You got the right kind of GPU!')"
      ],
      "id": "1D2hc2uLIe7W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwrlXjLytT_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9066206a-8763-4902-d011-eac24b5a3a8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "VwrlXjLytT_a",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htfx-OFUt4pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd819869-398b-4064-a7c7-392313ac0c35"
      },
      "source": [
        "import os\r\n",
        "os.chdir(\"/content/drive/MyDrive/Mask_RCNN\")\r\n",
        "print(os.getcwd())"
      ],
      "id": "htfx-OFUt4pw",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Mask_RCNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7O_fXHLG_hp"
      },
      "source": [
        "\r\n"
      ],
      "id": "v7O_fXHLG_hp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo4EPc9sNnvb"
      },
      "source": [
        ""
      ],
      "id": "Qo4EPc9sNnvb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axZC4ynWEsbn"
      },
      "source": [
        ""
      ],
      "id": "axZC4ynWEsbn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffKV5OfUu7n8"
      },
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "import json\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import skimage.draw\r\n",
        "\r\n"
      ],
      "id": "ffKV5OfUu7n8",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4PE0nHsAbCl"
      },
      "source": [
        "# Root directory of the project\r\n",
        "ROOT_DIR = os.path.abspath(\"/content/drive/MyDrive/Mask_RCNN\")\r\n",
        "\r\n",
        "# Import Mask RCNN\r\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\r\n",
        "from mrcnn.config import Config\r\n",
        "import mrcnn.utils as utils\r\n",
        "from mrcnn import model as modellib\r\n",
        "\r\n",
        "\r\n",
        "# Path to trained weights file\r\n",
        "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"weights.h5\")\r\n",
        "train_annotations_file = \"/content/drive/MyDrive/Mask_RCNN/dataset/train/instances.json\"\r\n",
        "train_image_dir = \"/content/drive/MyDrive/Mask_RCNN/dataset/train\"\r\n",
        "valid_annotations_file = \"/content/drive/MyDrive/Mask_RCNN/dataset/valid/instances.json\"\r\n",
        "valid_image_dir = \"/content/drive/MyDrive/Mask_RCNN/dataset/valid\"\r\n",
        "NUM_EPOCHS = 10\r\n",
        "MODEL_NAME = \"mask_v1\"\r\n",
        "# Directory to save logs and model checkpoints, if not provided\r\n",
        "# through the command line argument --logs\r\n",
        "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"
      ],
      "id": "y4PE0nHsAbCl",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "formal-judgment",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f90e3b-0777-4e29-d75b-69b06113c389"
      },
      "source": [
        "class Config(Config):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = MODEL_NAME\n",
        "\n",
        "    # We use a GPU with 12GB memory, which can fit two images.\n",
        "    # Adjust down if you use a smaller GPU.\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 3  # Background + insulator,tower,grids\n",
        "    #All of training images are 640X512\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 640\n",
        "    PR_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
        "    STEPS_PER_EPOCH = 100\n",
        "    VALIDATION_STEPS = 50\n",
        "    BACKBONE = 'resnet101'\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9\n",
        "    \n",
        "    \n",
        "    Config().display()\n"
      ],
      "id": "formal-judgment",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.9\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  640\n",
            "IMAGE_META_SIZE                16\n",
            "IMAGE_MIN_DIM                  512\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [640 640   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           mask_v1\n",
            "NUM_CLASSES                    4\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "PR_ANCHOR_SCALES               (32, 64, 128, 256, 512)\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                100\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "political-superintendent"
      },
      "source": [
        "############################################################\n",
        "#  Dataset\n",
        "############################################################\n",
        "\n",
        "class PowerlineDataset(utils.Dataset):\n",
        "\n",
        "    def load_data(self, dataset_dir, subset):\n",
        "        \"\"\"Load a subset of the dataset.\n",
        "        dataset_dir: Root directory of the dataset.\n",
        "        subset: Subset to load: train or val\n",
        "        \"\"\"\n",
        "        # Add classes. We have only 3 classes to add.\n",
        "        self.add_class(MODEL_NAME, 1, \"Tower\")\n",
        "        self.add_class(MODEL_NAME, 2, \"Insulator\")\n",
        "        self.add_class(MODEL_NAME, 3, \"Grids\")\n",
        "\n",
        "        # Train or validation dataset?\n",
        "        assert subset in [\"train\", \"valid\"]\n",
        "        dataset_dir = os.path.join(dataset_dir, subset)\n",
        "\n",
        "    def load_data(self, annotation_json, images_dir):\n",
        "        \"\"\" Load the powerline dataset from json\n",
        "        Args:\n",
        "            annotation_json: The path to the coco annotations json file\n",
        "            images_dir: The directory holding the images referred to by the json file\n",
        "        \"\"\"\n",
        "        # Load json from file\n",
        "        json_file = open(\"/content/drive/MyDrive/Mask_RCNN/dataset/train/instances.json\")\n",
        "        coco_json = json.load(json_file)\n",
        "        json_file.close()\n",
        "        \n",
        "        # Add the class names using the base method from utils.Dataset\n",
        "        \n",
        "        for category in coco_json['categories']:\n",
        "            class_id = category['1']\n",
        "            class_name = category['Tower']\n",
        "        self.add_class(source_name, class_id, class_name) \n",
        "            \n",
        "        \n",
        "        for category in coco_json['categories']:\n",
        "            class_id = category['2']\n",
        "            class_name = category['Insulator']\n",
        "            self.add_class(source_name, class_id, class_name)  \n",
        "             \n",
        "        for category in coco_json['categories']:\n",
        "            class_id = category['3']\n",
        "            class_name = category['Grids']\n",
        "            if class_id < 1:\n",
        "                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
        "                return\n",
        "            \n",
        "            \n",
        "        \n",
        "        \n",
        "        # Get all annotations\n",
        "        annotations = {}\n",
        "        for annotation in coco_json['annotations']:\n",
        "            image_id = annotation['image_id']\n",
        "            if image_id not in annotations:\n",
        "                annotations[image_id] = []\n",
        "            annotations[image_id].append(annotation)\n",
        "        \n",
        "        # Get all images and add them to the dataset\n",
        "        seen_images = {}\n",
        "        for image in coco_json['images']:\n",
        "            image_id = image['id']\n",
        "            if image_id in seen_images:\n",
        "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
        "            else:\n",
        "                seen_images[image_id] = image\n",
        "                try:\n",
        "                    image_file_name = image['file_name']\n",
        "                    image_width = image['width']\n",
        "                    image_height = image['height']\n",
        "                except KeyError as key:\n",
        "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
        "                \n",
        "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
        "                image_annotations = annotations[image_id]\n",
        "                 # Add the image using the base method from utils.Dataset\n",
        "                self.add_image(\n",
        "                    source=source_name,\n",
        "                    image_id=image_id,\n",
        "                    path=image_path,\n",
        "                    width=image_width,\n",
        "                    height=image_height,\n",
        "                    annotations=image_annotations\n",
        "                )\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\" Load instance masks for the given image.\n",
        "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
        "        Args:\n",
        "            image_id: The id of the image to load masks for\n",
        "        Returns:\n",
        "            masks: A bool array of shape [height, width, instance count] with\n",
        "                one mask per instance.\n",
        "            class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        image_info = self.image_info[image_id]\n",
        "        annotations = image_info['annotations']\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
        "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
        "            for segmentation in annotation['segmentation']:\n",
        "                mask_draw.polygon(segmentation, fill=1)\n",
        "                bool_array = np.array(mask) > 0\n",
        "                instance_masks.append(bool_array)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        mask = np.dstack(instance_masks)\n",
        "        class_ids = np.array(class_ids, dtype=np.int32)\n",
        "        \n",
        "        return mask, class_ids"
      ],
      "id": "political-superintendent",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1npHQZ02ST_b"
      },
      "source": [
        ""
      ],
      "id": "1npHQZ02ST_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "charitable-flower"
      },
      "source": [
        "def train(model):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    # Training dataset.\n",
        "    dataset_train = PowerlineDataset()\n",
        "    dataset_train.load_data(train_annotations_file, train_image_dir)\n",
        "    dataset_train.prepare()\n",
        "\n",
        "    # Validation dataset\n",
        "    dataset_val = PowerlineDataset()\n",
        "    dataset_val.load_custom(valid_annotations_file, valid_image_dir)\n",
        "    dataset_val.prepare()\n",
        "\n",
        "config=Config()\n",
        "model = modellib.MaskRCNN(mode = \"training\", config = Config(), model_dir = DEFAULT_LOGS_DIR)\n",
        "model.load_weights(COCO_WEIGHTS_PATH, by_name = True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n"
      ],
      "id": "charitable-flower",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hohqJDsGEK61"
      },
      "source": [
        " \r\n",
        " dataset_train = PowerlineDataset()\r\n",
        " dataset_val = PowerlineDataset()\r\n",
        " start_train = time.time()\r\n",
        " model.train(dataset_train, dataset_val, learning_rate = Config().LEARNING_RATE, epochs = NUM_EPOCHS, layers = 'heads')\r\n",
        " train(model)\r\n",
        " end_train = time.time()\r\n",
        " minutes = round((end_train - start_train)/60, 2)\r\n",
        " print(f'Training took {minutes} minutes')"
      ],
      "id": "hohqJDsGEK61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E829DBcGiXb3"
      },
      "source": [
        ""
      ],
      "id": "E829DBcGiXb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA_9CcK_LbO6"
      },
      "source": [
        "#save the trained model weights\r\n",
        "model.save(\"model.h5\")\r\n",
        "print(\"Saved model to disk\")"
      ],
      "id": "YA_9CcK_LbO6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toxq7wCgECsv"
      },
      "source": [
        "!curl -L \"https://app.roboflow.com/ds/P7QqBECzD6?key=0Fc8Gksff6\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "id": "toxq7wCgECsv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYORP1PpDuMa"
      },
      "source": [
        "def color_splash(image, mask):\r\n",
        "    \"\"\"Apply color splash effect.\r\n",
        "    image: RGB image [height, width, 3]\r\n",
        "    mask: instance segmentation mask [height, width, instance count]\r\n",
        "    Returns result image.\r\n",
        "    \"\"\"\r\n",
        "    # Make a grayscale copy of the image. The grayscale copy still\r\n",
        "    # has 3 RGB channels, though.\r\n",
        "    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\r\n",
        "    # Copy color pixels from the original color image where mask is set\r\n",
        "    if mask.shape[-1] > 0:\r\n",
        "        # We're treating all instances as one, so collapse the mask into one layer\r\n",
        "        mask = (np.sum(mask, -1, keepdims=True) >= 1)\r\n",
        "        splash = np.where(mask, image, gray).astype(np.uint8)\r\n",
        "    else:\r\n",
        "        splash = gray.astype(np.uint8)\r\n",
        "    return splash"
      ],
      "id": "QYORP1PpDuMa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joewQ1mgD0V0"
      },
      "source": [
        "def detect_and_color_splash(model, image_path=None, video_path=None):\r\n",
        "    assert image_path or video_path\r\n",
        "\r\n",
        "    # Image or video?\r\n",
        "    if image_path:\r\n",
        "        # Run model detection and generate the color splash effect\r\n",
        "        print(\"Running on {}\".format(args.image))\r\n",
        "        # Read image\r\n",
        "        image = skimage.io.imread(args.image)\r\n",
        "        # Detect objects\r\n",
        "        r = model.detect([image], verbose=1)[0]\r\n",
        "        # Color splash\r\n",
        "        splash = color_splash(image, r['masks'])\r\n",
        "        # Save output\r\n",
        "        file_name = \"splash_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\r\n",
        "        skimage.io.imsave(file_name, splash)\r\n",
        "    elif video_path:\r\n",
        "        import cv2\r\n",
        "        # Video capture\r\n",
        "        vcapture = cv2.VideoCapture(video_path)\r\n",
        "        width = int(vcapture.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n",
        "        height = int(vcapture.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n",
        "        fps = vcapture.get(cv2.CAP_PROP_FPS)\r\n",
        "\r\n",
        "        # Define codec and create video writer\r\n",
        "        file_name = \"splash_{:%Y%m%dT%H%M%S}.avi\".format(datetime.datetime.now())\r\n",
        "        vwriter = cv2.VideoWriter(file_name,\r\n",
        "                                  cv2.VideoWriter_fourcc(*'MJPG'),\r\n",
        "                                  fps, (width, height))"
      ],
      "id": "joewQ1mgD0V0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu53fGGqD4gA"
      },
      "source": [
        " count = 0\r\n",
        "        success = True\r\n",
        "        while success:\r\n",
        "            print(\"frame: \", count)\r\n",
        "            # Read next image\r\n",
        "            success, image = vcapture.read()\r\n",
        "            if success:\r\n",
        "                # OpenCV returns images as BGR, convert to RGB\r\n",
        "                image = image[..., ::-1]\r\n",
        "                # Detect objects\r\n",
        "                r = model.detect([image], verbose=0)[0]\r\n",
        "                # Color splash\r\n",
        "                splash = color_splash(image, r['masks'])\r\n",
        "                # RGB -> BGR to save image to video\r\n",
        "                splash = splash[..., ::-1]\r\n",
        "                # Add image to video writer\r\n",
        "                vwriter.write(splash)\r\n",
        "                count += 1\r\n",
        "        vwriter.release()\r\n",
        "    print(\"Saved to \", file_name)"
      ],
      "id": "Wu53fGGqD4gA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lined-question"
      },
      "source": [
        "dataset_train = PowerlineDataset()\n",
        "dataset = dataset_train\n",
        "image_ids = np.random.mtrand.RandomState.choice(image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset.load_image(image_id)\n",
        "    mask, class_ids = dataset.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)"
      ],
      "id": "lined-question",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "technical-hearing"
      },
      "source": [
        "    config = Config()\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                                  model_dir=MODEL_DIR)\n",
        "\n",
        "model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "        "
      ],
      "id": "technical-hearing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "figured-albania"
      },
      "source": [
        "# *** This training schedule is an example. Update to your needs ***\n",
        "    # Since we're using a very small dataset, and starting from\n",
        "    # COCO trained weights, we don't need to train too long. Also,\n",
        "    # no need to train all layers, just the heads should do it.\n",
        "    print(\"Training network heads\")\n",
        "    start_train = time.time()\n",
        "model.train(dataset_train, dataset_val,\n",
        "                learning_rate=config.LEARNING_RATE,\n",
        "                epochs=100,\n",
        "                layers='heads')\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "print(f'Training took {minutes} minutes')"
      ],
      "id": "figured-albania",
      "execution_count": null,
      "outputs": []
    }
  ]
}